{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def wordlist(): # reformatting the .txt file of the book into a complete list of words\n",
    "    GRE_file = open('GRE.txt').readlines()\n",
    "    alphabet = list(map(chr, range(97, 123)))\n",
    "    GRE_wordlist = []\n",
    "    for i, word in enumerate(GRE_file):\n",
    "        if word == \"\\n\": # identifying new words using the fact that a word constitutes a line itself\n",
    "            pot_new = GRE_file[i + 1].strip().replace(\"\\n\", \"\").replace(\".\", \"\")\n",
    "            n = 0\n",
    "            for characters in pot_new:\n",
    "                if alphabet.count(characters) == 0:\n",
    "                    n = 1\n",
    "            if n == 0:\n",
    "                GRE_wordlist.append(pot_new)\n",
    "    return GRE_wordlist[1:len(GRE_wordlist)-1]\n",
    "\n",
    "words = wordlist()\n",
    "pd.DataFrame(words).to_csv('words.csv',index=False)\n",
    "primary_sentence_matrix= []\n",
    "for i,word in enumerate(words):\n",
    "    website = requests.get(f\"https://www.merriam-webster.com/dictionary/{word}\").text\n",
    "    web_parsed = bs(website,\"lxml\")\n",
    "    try:\n",
    "        sent_sect = web_parsed.find(\"div\", id=\"examples\")\n",
    "\n",
    "        # this line identifies the section of the webpage that provides sample sentences\n",
    "\n",
    "        primary_sent_ex = sent_sect.find_all(\"span\", class_=\"d-block t has-aq\")\n",
    "\n",
    "        # if modifications are made to the website, change the argument given to class_ by inspecting the webpage\n",
    "\n",
    "        primary_sent_list = []\n",
    "        if primary_sent_ex:\n",
    "            for sentence in primary_sent_ex:\n",
    "                sentence = sentence.text\n",
    "                primary_sent_list.append(sentence.replace(f'{word}', f'__{word}__'))\n",
    "            primary_sentence_matrix.append(primary_sent_list)\n",
    "        else: # in case the first method fails to identify any sentence\n",
    "            secondary_sent_ex = sent_sect.find_all('span', class_='sub-content-thread ex-sent sents')\n",
    "            for sentence in secondary_sent_ex:\n",
    "                sentence = sentence.text.replace(f'{word}', f'__{word}__').replace('\\n', '').replace('\\t', '') + ' (sourced from Web)'\n",
    "                primary_sent_list.append(sentence)\n",
    "            primary_sentence_matrix.append(primary_sent_list)\n",
    "        #time.sleep(3)\n",
    "    except Exception:\n",
    "        primary_sentence_matrix.append([\"No sentence.\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "pd.DataFrame(primary_sentence_matrix).transpose().to_csv(\"outputs.csv\", index=False, columns=words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}